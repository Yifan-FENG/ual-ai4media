{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://www.tensorflow.org/tutorials/audio/transfer_learning_audio\n",
    "#Google Colab alternative: https://colab.research.google.com/drive/1b1Ade24iVxJHhgi32ODMsjfQfXcoA70c?usp=sharing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53670ba5",
   "metadata": {},
   "source": [
    "# Working with a Pretrained YAMNet\n",
    "\n",
    "## YAMNet\n",
    "\n",
    "YAMNet is a model provided by Tensorflow trained on the [AudioSet](https://research.google.com/audioset/) environmental sounds dataset. \n",
    "\n",
    "You can think of it as analogous to ``ImageNet`` but for audio.  \n",
    "\n",
    "```\n",
    "AudioSet consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos\n",
    "```\n",
    "\n",
    "We're going to look at \n",
    "\n",
    "\n",
    "* What are the labels it gives to new Youtube videos? \n",
    "\n",
    "\n",
    "* Using the ``embeddings`` it generates (think ``word embeddings`` from NLP) to train our own audio classifiers. This is conceptually similar to the transfer learning we did in the image domain, but implemented slightly differently. \n",
    "\n",
    "\n",
    "### Install ```tensorflow_hub```\n",
    "\n",
    "First we need to install ``tensorflow_hub``, this is a package which allow us to access some **pretrained models** from Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4abac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888147a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4399247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get class names\n",
    "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\n",
    "class_names =list(pd.read_csv(class_map_path)['display_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c6ffb",
   "metadata": {},
   "source": [
    "### Ripping audio from Youtube Videos \n",
    "\n",
    "#### Youtube-dl\n",
    "\n",
    "``youtube-dl`` is a great tool for extracting videos from Youtube. You can install it using ``homebrew`` (which you hopefully installed in Week 2!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install youtube-dl (Mac OSX only)\n",
    "!brew install youtube-dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get video from youtube, extract audio and trim\n",
    "def get_audio(youtube_id, file_length=60):\n",
    "\n",
    "    output_filename = youtube_id + \".wav\"\n",
    "    \n",
    "    #Remove existing files from previous runs\n",
    "    !rm youtube_audio.m4a\n",
    "    !rm full.wav\n",
    "    !rm $output_filename\n",
    "    \n",
    "    #Get youtube video\n",
    "    !youtube-dl -ci -f \"bestaudio[ext=m4a]\" $youtube_id -o 'youtube_audio.m4a'\n",
    "    \n",
    "    #Extract audio\n",
    "    !ffmpeg -i 'youtube_audio.m4a' -ac 2 -f wav full.wav\n",
    "    \n",
    "    #Trim\n",
    "    !ffmpeg -y -ss $file_length -i full.wav -t $file_length -c copy $output_filename\n",
    "    \n",
    "    #Read into librosa and return audio data (samples)\n",
    "    y,sr = librosa.load(output_filename,sr=16000)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22e98f",
   "metadata": {},
   "source": [
    "### Try it out\n",
    "\n",
    "Put the **id of a youtube video** into the function below. The first part will get the audio, the second will run it through the ``YamNet`` and show the classification of the audio at various timestamps. \n",
    "\n",
    "#### Warning \n",
    "\n",
    "We have to download the **whole video**, then we trim it down. So probably, pick a video that is between 1-3 mins\n",
    "\n",
    "#### Example\n",
    "\n",
    "For a youtube video address like this: https://www.youtube.com/watch?v=Ptxjrmqo2Xo, we only need this bit: Ptxjrmqo2Xo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first minute of the audio\n",
    "file_length = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get audio and trim to file_length\n",
    "audio_class_one = get_audio(\"Ptxjrmqo2Xo\", file_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run through the yamNet model and get scores for first minute in timestamps of half a second\n",
    "#Scores is the output of the final layer, the probability of the audio being one of the classes\n",
    "scores, embeddings_class_one, spec = yamnet_model(audio_class_one)\n",
    "interval = np.round(file_length/len(scores),1)\n",
    "all_sounds = [str(i*interval) + \"s -> \" + class_names[tf.argmax(score)] for i,score in enumerate(scores)]\n",
    "#Getting the predicted label for each timestamp\n",
    "all_sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3163b4",
   "metadata": {},
   "source": [
    "## Building an Audio Classifier\n",
    "\n",
    "### Embeddings \n",
    "\n",
    "You'll notice that as well as a ``scores`` data that gets returned by the model (which we use to check what labels have been applied to that audio buffer), we also get something called ``embeddings``. \n",
    "\n",
    "Similarly to word embeddings used in NLP tasks that provide a **denser, continuous** representation of text data, we can use a similar approach to get a vector of **1024 numbers** that represent each audio frame. \n",
    "\n",
    "So, instead of taking the model and retraining it like we did with **image classifier**, here, we **first run each audio frame through the model** and get the **embeddings**. This then becomes the features that we use to save in the dataset. \n",
    "\n",
    "We can then build our own simple classifier based on this dataset, with the **embedding** being a highly optimised representation of each audio buffer when doing audio classification tasks that are similiar that the one the original YAMNet was trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c40af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get second audio class\n",
    "audio_class_two = get_audio(\"D6Cwi-jMzdc\",file_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee756a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_2, embeddings_class_two, spec = yamnet_model(audio_class_two)\n",
    "interval = np.round(file_length/len(scores_2),1)\n",
    "all_sounds = [str(i*interval) + \"s -> \" + class_names[tf.argmax(score)] for i,score in enumerate(scores_2)]\n",
    "all_sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89970b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels (output)\n",
    "num_class_one = embeddings_class_one.shape[0]\n",
    "num_class_two = embeddings_class_two.shape[0]\n",
    "#class_one = 0, class_two = 1\n",
    "labels = tf.concat([tf.zeros(num_class_one), tf.ones(num_class_two)],0)\n",
    "\n",
    "#Features/Embeddings (input)\n",
    "features = tf.concat([embeddings_class_one,embeddings_class_two],0)\n",
    "\n",
    "#Test-Train split\n",
    "#20% for validation testing, the rest for training\n",
    "X_train, X_val, y_train, y_val = train_test_split(features.numpy(), \n",
    "                                                    labels.numpy(),\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "#Build dataset\n",
    "train_ds = tf.data.Dataset.from_tensors((X_train,y_train))\n",
    "val_ds = tf.data.Dataset.from_tensors((X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db176dc",
   "metadata": {},
   "source": [
    "### Training Callbacks and Early Stopping\n",
    "\n",
    "We build up out ``Sequential()`` model much the same as we have done in the past, making the sure the ``Input`` layer is the right size to accept our embeddings (1024 values for each audio buffer). \n",
    "\n",
    "We also add in an extra function called a ``callback``. This is a function that gets called **at the end of every epoch**\n",
    "\n",
    "We can make our own custom functions, or use some of the built in ones that come with ``Keras``. Here we use ``EarlyStopping()``, which checks conditions at each epoch and decides whether we should continue with training\n",
    "\n",
    "* ``monitor``: Tells us which metric to keep tabs on whilst training. Here, we pick ``loss`` \n",
    "\n",
    "\n",
    "* ``patience``: This tells us how many epochs to wait once the ``loss`` has **stopped decreasing** \n",
    "\n",
    "\n",
    "* ``restore_best_weights``: This means that after **early stopping**, we keep the weights that gave us the lowest loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917dfa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59149f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make model\n",
    "embedding_size = embeddings_class_one.shape[1]\n",
    "classifier = Sequential([\n",
    "    #the input layer is the size of the embeddings\n",
    "    layers.Input(shape=(embedding_size), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "], name='my_model')\n",
    "\n",
    "classifier.summary()\n",
    "classifier.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train new classifier with a callback called at the end of every epoch\n",
    "history = classifier.fit(train_ds,\n",
    "                         validation_data=val_ds,\n",
    "                         epochs=20,\n",
    "                         callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369414a3",
   "metadata": {},
   "source": [
    "## Tasks \n",
    "\n",
    "1. Try some different Youtube videos and check out the timestamped labels, do these seem correct?\n",
    "\n",
    "\n",
    "2. Download another Youtube video and try a binary classifier, does it work well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb9ed43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
