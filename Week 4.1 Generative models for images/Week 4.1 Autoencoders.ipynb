{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##adpated from https://keras.io/examples/generative/vae/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a84011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c745d06",
   "metadata": {},
   "source": [
    "# Week 4.1 Autoencoders\n",
    "\n",
    "For our generative models, we want something that will\n",
    "\n",
    "1. Compress our original **high dimensional data** into a lower dimensional ``representation`` or ``latent`` vector\n",
    "\n",
    "\n",
    "2. Use this ``latent vector`` to generate **new images** that are plausibly from the original set, but also intersting in their variety. \n",
    "\n",
    "\n",
    "To do this, we will build **two neural networks** to do each task, and then train them both **at the same time**!.\n",
    "\n",
    "These neural networks will use very similar structures to those we've seen before (``Dense`` layers and ``Convolution`` operations)\n",
    "\n",
    "\n",
    "### The Encoder \n",
    "\n",
    "Using convolution laters, this first model will take input image (first we'll be working with some ``28x28`` grayscale pictures), and output a ``2 dimensional vector``. \n",
    "\n",
    "This will place the image somewhere in latent space!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ab865",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dims = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d87ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "encoder = Sequential([\n",
    "    layers.Input(shape = image_dims),\n",
    "    layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    ##Output is two numbers\n",
    "    layers.Dense(latent_dim)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24179e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec7370",
   "metadata": {},
   "source": [
    "### The Decoder \n",
    "\n",
    "The second model will take a ``2 dimensional vector`` and convert this back into a ``28x28`` black and white image!\n",
    "\n",
    "The structure of the ``Encoder`` and ``Decoder`` don't have to match, however....\n",
    "\n",
    "\n",
    "* The input of the ``Encoder`` and the output of the ``Decoder`` **must have the same shape**\n",
    "\n",
    "\n",
    "* The input of the ``Decoder`` and the output of the ``Encoder`` **must have the same shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dims = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679bbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = int(image_dims[0]/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Sequential([\n",
    "    keras.Input(shape=(latent_dim,)),\n",
    "    layers.Dense(quarter * quarter * 64, activation=\"relu\"),\n",
    "    layers.Reshape((quarter, quarter, 64)),\n",
    "    layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974bc02",
   "metadata": {},
   "source": [
    "### Putting it together \n",
    "\n",
    "We combine these two models into a final ``Sequential``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8707a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = Sequential([encoder, decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356088e",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "We're first going to try and learn from a dataset consisting of **hand written digits**. This is called [MNIST](http://yann.lecun.com/exdb/mnist/) and has been a mainstay of computer vision research for decades. \n",
    "\n",
    "We can load this directly from ``Keras`` and it has **70,000** examples in all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load in images\n",
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "##Join test and train sets together\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "##Normalise down to 0-1\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a08012",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_digits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687e5bc",
   "metadata": {},
   "source": [
    "### Custom Loss Functions\n",
    "\n",
    "So **how do we train these models?**\n",
    "\n",
    "Our broader goal in terms of generative modelling is to have something that can **generate new images similar to the original set**.\n",
    "\n",
    "In practical terms, if all is working well, the model will be able to **accurately reconstruct** the input image when it comes out of the ``decoder``. \n",
    "\n",
    "This means the dataset we use is **the same for both input and output**.\n",
    "\n",
    "We define our **loss function** as the **difference** between the ``original image`` and ``decoded image``.\n",
    "\n",
    "Since this isn't built into ``Keras``, we define our own ``custom function``. It is called every time a **forward pass** has been executed containing the output of the model, and the expected output from the dataset. \n",
    "\n",
    "We use this to calculate the loss (however we choose fit!), and return a number from this function. This is then used by ``Keras`` to update the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our custom function \n",
    "def reconstruction_loss(original, decoded):\n",
    "    difference = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.mean_squared_error(original, decoded), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model (giving custom loss function)\n",
    "auto_encoder.compile(optimizer=keras.optimizers.Adam(), loss = reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e54032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train (x and y is the **same data**)\n",
    "auto_encoder.fit(x = mnist_digits, y = mnist_digits, epochs=30, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851cc73",
   "metadata": {},
   "source": [
    "## Exploring the Latent Space \n",
    "\n",
    "So we've trained the model. Lets generate some digits!\n",
    "\n",
    "First we'll pick a random point (``z``), and use the ``decoder`` to generate image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28323c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate random point\n",
    "z = (np.random.random((1,2))*10)-5\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9baf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decode image\n",
    "x_decoded = decoder.predict(z)\n",
    "digit = x_decoded[0].reshape(image_dims[0], image_dims[1])\n",
    "plt.imshow(digit,cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac59d5",
   "metadata": {},
   "source": [
    "### All of Latent space!\n",
    "\n",
    "Now lets see what that looks like in a grid. \n",
    "\n",
    "We can see how things that are near to each other in latent space have similar characteristics!\n",
    "\n",
    "The learning process has found a representation that has organised the numbers based on some of the underlying features they possess!\n",
    "\n",
    "In an ideal world, all of our digit classes would be equally represented so when we sample it to generate new images, we get an accurate representation of the original set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962bf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions from cci_autoencoders.py\n",
    "from cci_autoencoders import plot_label_clusters\n",
    "from cci_autoencoders import plot_latent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55decd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b8e4d",
   "metadata": {},
   "source": [
    "### Plotting the original dataset\n",
    "\n",
    "Now lets look at the original dataset. We can use the ``encoder`` to take each image (with the colour representing which digit it is) and plot them in latent space. \n",
    "\n",
    "In an ideal world, these would be well spread across the space. However, we can see that the area for 0's and 1's is much bigger and more separated than other digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "z_mean = encoder.predict(x_train)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_train)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a772f",
   "metadata": {},
   "source": [
    "## Variational Autoenconders\n",
    "\n",
    "There are infact a couple of tweaks to the standard autoencoder setup that allow for both **better quality images** and a **more interesting** and **spread out** latent space. In reality, **Variational Autoencoders** are what anyone would actually use for any practical purposes. \n",
    "\n",
    "We've already learned so much new stuff (like total champs), so we won't cover this in much detail now. The main intuition to take is that instead of encoding and decoding each image as **a single point** in latent space, we sample from a **normal distribution** around a given point in latent space (see picture). \n",
    "\n",
    "\n",
    "This means the model is slightly different, and we have to account for some extra metrics into the **loss function**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a382a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cci_autoencoders\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cci_autoencoders import init_VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1657118",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = init_VAE(latent_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca580a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(mnist_digits, epochs=30, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(vae.decoder, scale = (-5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92337ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "z_mean, _, _ = vae.encoder.predict(x_train)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_train)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"z[0]\")\n",
    "plt.ylabel(\"z[1]\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('aim_week3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7f6b83f8c63488afc271070561874a11a039d471dfbe07d7ca18d88af4be80a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
