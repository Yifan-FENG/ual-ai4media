{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks from Scratch \n",
    "\n",
    "Notebook originally by Louis McCallum 2022, updated by Irini Kalaitzidi 2023\n",
    "\n",
    "Neural Networks consist of the following components:\n",
    "\n",
    "* An input layer\n",
    "\n",
    "* An arbitrary amount of hidden layers\n",
    "\n",
    "* An output layer\n",
    "\n",
    "* A set of weights and biases between each layer\n",
    "\n",
    "* A choice of activation function for each hidden layer.\n",
    "    \n",
    "    \n",
    "We are going to make a Neural Network that has \n",
    "\n",
    "* 2 Inputs (audio features)\n",
    "* 1 Hidden layer (with 3 neurons)\n",
    "* 1 Output (type of drum)\n",
    "\n",
    "![fishy](../images/audio_net.png)\n",
    "\n",
    "We have two sets of weights,  \n",
    "\n",
    "* Input (+ bias) -> Hidden layer\n",
    "    * [num_inputs + 1, num_hidden_nodes] = 3 x 3\n",
    "* Hidden Layer (+ bias) -> Output\n",
    "    * [num_hidden_nodes + 1, num_outputs] = 4 x 1\n",
    "\n",
    "**Total 13 parameters**\n",
    "\n",
    "### Randomly initiate weights\n",
    "\n",
    "First we have to randomly initialise the weights, otherwise training will not work. We could, for example, start them all at zero, but for reasons that will become apparent, this is a bad idea. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the structure of our network (how many neurons in each layer)\n",
    "#Get some data (2 inputs)\n",
    "x = np.array([[3,4]])\n",
    "input_size = x.shape[1]\n",
    "hidden_size = 3\n",
    "\n",
    "#Randomly initiate the weights\n",
    "hidden_weights = np.random.rand(input_size + 1, hidden_size) \n",
    "output_weights = np.random.rand(hidden_size + 1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_weights.shape, output_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Activations\n",
    "\n",
    "We also need to hold the outputs (activations) of each neuron somewhere.\n",
    "\n",
    "For the hidden layer, there will be **3** outputs.\n",
    "\n",
    "For the output layer, there will be **1** ouputs.\n",
    "\n",
    "We initialise these to be zero at beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = np.zeros((hidden_size, 1))\n",
    "output = np.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Forwards Pass\n",
    "\n",
    "Now we pump some numbers into our network and get an output out the otherside. This is known as the **forwards pass**!\n",
    "\n",
    "Each **input neuron** is connected to each **hidden layer neuron**, with a corresponding weight. We multiply each input by the weight, **sum** up all the inputs for each neuron and then pass through the **sigmoid function**.\n",
    "\n",
    "### Matrix Multiplication \n",
    "\n",
    "Lets take a quick stop to remember our linear algebra. When we multiply two matrices together, we take the **dot product** of each row of the first matrix and each column of the second matrix. And the **dot product** is where we multiply two sets of numbers together then sum. Sounds familiar? \n",
    "\n",
    "![fishy](../images/matrix_mul.png)\n",
    "\n",
    "### The hidden neuron activations\n",
    "\n",
    "If the input matrix is **1 x 2**, plus a bias term **1**: ``x = [[3, 4, 1]]``\n",
    "and the ``hidden_weights`` matrix is **3 x 3**, (each row representing the weights connecting a single input neuron  to each neuron in the hidden layer),\n",
    "then we can multiply the two together to get a **1 x 3** matrix containing the sums of all the connections.\n",
    "We then run this through the **sigmoid activation function**.\n",
    "\n",
    "Practically speaking, we take the first row of the input ``[3,4,1]`` and the first column of the ``hidden_weights``, multiply them together **element-wise**, then **sum**. Once we have **added the bias** and passed everything through the **sigmoid function**, we get the **output** of the first hidden neuron.\n",
    "\n",
    "```\n",
    "\n",
    "hidden_layer[0] = sigmoid(input[0][0] * hidden_weight[0][0] + input[0][1] * hidden_weight[1][0] + hidden_weight[2][0])\n",
    "hidden_layer[1] = sigmoid(input[0][0] * hidden_weight[0][1] + input[0][1] * hidden_weight[1][1] + hidden_weight[2][1])\n",
    "hidden_layer[2] = sigmoid(input[0][0] * hidden_weight[0][2] + input[0][1] * hidden_weight[1][2] + hidden_weight[2][2])\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add one to all rows for bias term\n",
    "def with_bias(x):\n",
    "    x = np.array(x)\n",
    "    ones = np.ones((1,x.shape[0])).T\n",
    "    return np.hstack((x, ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Define the sigmoid function \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Forward Pass\n",
    "x = with_bias([[3,4]])\n",
    "print(\"input \\n\",x)\n",
    "print(\"hidden layer weights \\n\",hidden_weights)\n",
    "hidden_layer = sigmoid(x @ hidden_weights)\n",
    "hidden_layer = with_bias(hidden_layer)\n",
    "print(\"hidden layer outputs \\n\",hidden_layer)\n",
    "output = sigmoid(hidden_layer @ output_weights)\n",
    "print(\"output layer weights \\n\",output_weights)\n",
    "print(\"output \\n\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also works with multiple inputs at once\n",
    "x = with_bias([[3,4],[5,6],[6,1]])\n",
    "\n",
    "print(\"inputs \\n\",x)\n",
    "#Forward Pass\n",
    "hidden_layer = sigmoid(x @ hidden_weights)\n",
    "hidden_layer = with_bias(hidden_layer)\n",
    "## We get 3 sets of hidden layer outputs from 3 sets of inputs\n",
    "print(\"hidden layer outputs \\n\",hidden_layer)\n",
    "output = sigmoid(hidden_layer @ output_weights)\n",
    "## And 3 outputs\n",
    "print(\"output \\n\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset \n",
    "\n",
    "We are going to take some audio files of two types of drum hits: **snares** and **hihats**. \n",
    "\n",
    "If our model has only **2 inputs**, we need to take each audio file and get **2 features** out of them that accurately represent the similarities and differences between the two classes. \n",
    "\n",
    "We use the **librosa** library to get the **spectral flatness** and **spectral centroid**. These tell us something about the frequencies present in a given window in the file. \n",
    "\n",
    "Lets take a look at some example files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load audio file\n",
    "hh, sr = librosa.load(\"../audio/drum_hits/hihat_hatsH1.WAV\")\n",
    "sn, sr = librosa.load(\"../audio/drum_hits/snare_SNR111.WAV\")\n",
    "#Get features for whole audio\n",
    "flatness_hh = librosa.feature.spectral_flatness(y=hh,n_fft=1028)\n",
    "cent_hh = librosa.feature.spectral_centroid(y=hh, sr=sr,n_fft=1028)\n",
    "flatness_sn = librosa.feature.spectral_flatness(y=sn,n_fft=1028)\n",
    "cent_sn = librosa.feature.spectral_centroid(y=sn, sr=sr,n_fft=1028)\n",
    "fig, ax = plt.subplots(ncols=2,figsize=(12,6))\n",
    "#plot\n",
    "ax[0].plot(flatness_hh[0], label = \"hihat\")\n",
    "ax[0].plot(flatness_sn[0], label = \"snare\")\n",
    "ax[1].plot(cent_hh[0], label = \"hihat\")\n",
    "ax[1].plot(cent_sn[0], label = \"snare\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[0].set_title(\"Spectral Flatness\")\n",
    "ax[1].set_title(\"Spectral Centroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the mean \n",
    "\n",
    "Each feature gives us 5-6 values (depending on the length of the audio file). We only want 2 inputs per audio file!\n",
    "\n",
    "We can see that there is some change over the file but probably the **mean** of each **feature** will be enough to differentiate between the two types of drum.\n",
    "\n",
    "### Getting the whole set \n",
    "\n",
    "Below we scan the whole ``../audio/drum_hits`` folder and get the **mean spectral flatness** and **mean spectral centroid** for every file. We now have **2 features** that represent each audio file, alongside a label telling us which class they are in. \n",
    "\n",
    "### Normalising \n",
    "\n",
    "We also **normalise** each feature so they are between **0 and 1** using the ``minmax_scale`` function from the ``sklearn`` library. Before, the centroid feature went up to **7000**! Normalising data is common when training neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dataset\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "#Walk through all the files\n",
    "for root, dirs, files in os.walk(\"../audio/drum_hits\", topdown=False):\n",
    "    for name in files:\n",
    "        ## Get all the \".wav\" files\n",
    "        if \".wav\" in name.lower():\n",
    "            f = os.path.join(root, name)\n",
    "            #Class 0 is snares, class 1 is hihats \n",
    "            label = 0 if \"snare\" in name else 1\n",
    "            #Load audio file\n",
    "            audio, sr = librosa.load(f)\n",
    "            #Get features for whole audio\n",
    "            flatness = librosa.feature.spectral_flatness(y=audio,n_fft=1028)\n",
    "            cent = librosa.feature.spectral_centroid(y=audio, sr=sr,n_fft=1028)\n",
    "            #Get mean across the audio\n",
    "            features = [np.mean(flatness), np.mean(cent)] \n",
    "            #Add to dataset\n",
    "            x.append(features)\n",
    "            y.append([label])\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "#Scale to between 0 and 1\n",
    "x = minmax_scale(x)\n",
    "y = minmax_scale(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the features\n",
    "\n",
    "We can plot our two features against each other and label the separate classes in different colours. These look like good features because the two classes look reasonably separable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check out the classes and features \n",
    "fig,ax = plt.subplots()\n",
    "scatter = ax.scatter(x[:,0],x[:,1],c=y)\n",
    "legend1 = ax.legend(*scatter.legend_elements(), title=\"Drum Type\")\n",
    "ax.add_artist(legend1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the Network!\n",
    "\n",
    "We can now take all of our drums (represented by 2 features each) and run them through the network. Again, the output will be rubbish because the weights are **random**.\n",
    "\n",
    "Here, we just pass **all the examples** all at once. We can do this because the dataset is reasonably small. When you have larger datasets, it might be necessary to pass it in **batches** (e.g. 32 at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Forward Pass\n",
    "hidden_layer = sigmoid(with_bias(x) @ hidden_weights)\n",
    "output = sigmoid(with_bias(hidden_layer) @ output_weights)\n",
    "#we get 86 outputs back!\n",
    "len(output), output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting good weights\n",
    "\n",
    "We have a **Neural Network!**. But it doesn't actually do anything useful (yet). \n",
    "\n",
    "So, lets get some labeled data and use it to **update the weights** so that our network can give sensible predictions/labels when it sees **new data**. \n",
    "\n",
    "### Training \n",
    "\n",
    "The way we get better weights is by **training** and in the case of neural networks, training is a process of **updating the weights**. We first work out the **loss**, so that we know how good our model is. Then we are going to use this information to nudge the weights slightly, hopefully improving performance. \n",
    "\n",
    "If we do this enough times, we hopefully end up with weights that will transform our **input (audio features)** into an **output (type of drum)** that correctly reflects the phenomena we are trying model!\n",
    "\n",
    "Essentially, you can think of our task as finding the **parameters** (weights) that **minimise** (get the lowest value of) the **loss function** (how bad our model is predicting labels). In this context, a low loss is a good thing!\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "For that, we are going to use the **gradient descent** algorithm, an optimisation algorithm for finding the (local) minimum loss value.\n",
    "\n",
    "![fishy](../images/grad-descent.png)\n",
    "\n",
    "Image from https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "\n",
    "We'll cover this in more detail in the upcoming lectures, but the basics are: \n",
    "\n",
    "1. Calculate the error. We will use the **mean squared error** as our **loss function**. This means that we find the difference between the output and the target label, we square it, and then find the average across all the examples.\n",
    "\n",
    "2. For each weight in the network, determine the gradient of the loss function. This tells us the direction in which we need to head.\n",
    "\n",
    "3. Update each weight using this gradient multiplied by a **learning rate**. Higher learning rate means more change each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cci_gradients import get_gradients_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Backwards Pass\n",
    "#Get gradients for the mean squared error loss function (using chain rule, don't worry about this yet!)\n",
    "learning_rate = 0.05\n",
    "hidden_layer = sigmoid(with_bias(x) @ hidden_weights)\n",
    "output = sigmoid(with_bias(hidden_layer) @ output_weights)\n",
    "hidden_gradients,output_gradients = get_gradients_bias(with_bias(x), y, with_bias(hidden_layer), output, hidden_weights, output_weights, learning_rate)\n",
    "\n",
    "#update the weights with the gradient (slope) of the loss function and learning rate\n",
    "print(\"one gradient for each weight:\")\n",
    "print(hidden_weights.shape,hidden_gradients.shape)\n",
    "print(output_weights.shape,output_gradients.shape)\n",
    "hidden_weights = hidden_weights - (hidden_gradients * learning_rate)\n",
    "output_weights = output_weights - (output_gradients * learning_rate)\n",
    "#Mean Squared Error for the batch\n",
    "error = np.mean(np.square(y - output))\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside (Randomly Initialised Weights)\n",
    "\n",
    "If we had initialised the weight all to zero (or any same number), then the update would be the same for every neuron! This would be the same as having **only one neuron** in each layer, which gets rid of many of the advantages of multi-layered networks.\n",
    "\n",
    "\n",
    "### Batches\n",
    "\n",
    "Couple of final things before we put it all together:\n",
    "\n",
    "**One Epoch** is what we call one training cycle with all the data.\n",
    "\n",
    "We can chose how much data to use at once:\n",
    "\n",
    "* Batch Gradient Descent gives the whole dataset at once. This means we optimise the weights based on every example at once.\n",
    "\n",
    "* Stochastic Gradient Descent uses one (randomly selected) example at a time to update the weights. \n",
    "\n",
    "* Mini-batch Gradient Descent takes smaller sections of the whole dataset at a time.\n",
    "\n",
    "In the code below you can specify the batch size.\n",
    "\n",
    "\n",
    "# Putting it all together \n",
    "\n",
    "1. Initialise the weights to random numbers\n",
    "\n",
    "2. Split the training set into batches (anywhere from 1 -> whole dataset)\n",
    "\n",
    "3. Forward Pass to get the outputs\n",
    "\n",
    "4. Compare against expected outputs, find the error (loss function) and the gradient for each weight\n",
    "\n",
    "5. Update the weights using gradients and learning rate \n",
    "\n",
    "6. Go back to 3 and repeat ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "##How many hidden neurons in the hidden layer?\n",
    "input_size = x.shape[1]\n",
    "hidden_size = 3\n",
    "\n",
    "hidden_weights = np.random.randn(input_size + 1, hidden_size) \n",
    "output_weights = np.random.randn(hidden_size + 1, y.shape[1])    \n",
    "\n",
    "hidden_layer = np.zeros((hidden_size, y.shape[1]))\n",
    "output = np.zeros(y.shape[1])\n",
    "\n",
    "#Parameters (batch size, learning rate)\n",
    "num_examples = x.shape[0]\n",
    "batch_size = 1\n",
    "batches_per_epoch = int(np.ceil(num_examples / batch_size))\n",
    "epochs = 200\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "#Every epoch is every example once\n",
    "for i in range(epochs):\n",
    "    start = 0;\n",
    "    errors = []\n",
    "    for i in range(batches_per_epoch):\n",
    "        \n",
    "        #Get new batch of examples\n",
    "        end = start + batch_size\n",
    "        end = np.min([end,num_examples])\n",
    "        #Batch inputs\n",
    "        batch_x = x[start:end]\n",
    "        #Batch output labels\n",
    "        batch_y = y[start:end]\n",
    "\n",
    "        #Forwards pass\n",
    "        hidden_layer = sigmoid(with_bias(batch_x) @ hidden_weights)\n",
    "        output = sigmoid(with_bias(hidden_layer) @ output_weights)\n",
    "        \n",
    "        #Mean Squared Error for the batch\n",
    "        error = np.mean(np.square(batch_y - output))\n",
    "        hidden_gradients,output_gradients = get_gradients_bias(with_bias(batch_x), batch_y, with_bias(hidden_layer), output, hidden_weights, output_weights, learning_rate)\n",
    "        #update the weights with the gradient (slope) of the loss function and learning rate\n",
    "        hidden_weights = hidden_weights - (hidden_gradients * learning_rate)\n",
    "        output_weights = output_weights - (output_gradients * learning_rate)\n",
    "        \n",
    "        errors.append(error)\n",
    "        start += batch_size\n",
    "        \n",
    "    #Print average error for each epoch\n",
    "    print(np.mean(errors))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare against non normalised features \n",
    "\n",
    "Why do you think it is much worse?\n",
    "\n",
    "## Equivalent Keras Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 3\n",
    "learning_rate = 0.1\n",
    "epochs = 200\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_size, activation='sigmoid', input_dim=input_size))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = SGD(learning_rate=learning_rate)\n",
    "model.compile(opt, loss='mse',metrics=['accuracy'])\n",
    "model.fit(x, y, epochs=epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving forwards\n",
    "\n",
    "We just saw **mean squared error** loss function used with a **sigmoid** activation function on both the hidden layer and the output layer, but this is not always the right choice. \n",
    "\n",
    "As we move through the unit, we'll see lots of different **loss functions** and **activation functions**. However, the basic principles of optimising a neural network will remain broadly similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Tasks\n",
    "\n",
    "### Networks and Weights \n",
    "\n",
    "1. Given a network with 3 inputs, 1 hidden layer with 6 neurons and 2 outputs, how many parameters are there in the network in total?\n",
    "\n",
    "2. Given a network with 4 inputs, 1 hidden layer with 6 neurons, 1 hidden layer with 3 neurons and 1 output, how many parameters are there in the network in total? \n",
    "\n",
    "3. If I have a network with 3 inputs and a hidden layer with 4 neurons: \n",
    "\n",
    "    * What is the size of my ``hidden_weights`` matrix?\n",
    "    \n",
    "    * When I use matrix multiplication to combine the following input dataset with the hidden weights matrix, what is the shape of the result? What do these numbers represent?\n",
    "    \n",
    "     ```\n",
    "     inputs = [\n",
    "         [2,3,4],\n",
    "         [0.6,70,0.3],\n",
    "         [6,5,4],\n",
    "         [0.4,5,6],\n",
    "     ]\n",
    "     ```\n",
    "\n",
    "### Audio Features\n",
    "\n",
    "\n",
    "* The features we have picked make for quite separable data. Try the following different features and see how it updates the graph of features underneath it. Do they seem like they would be good features? Why not?\n",
    "\n",
    "    * Standard deviation of spectral centroid and spectral flatness\n",
    "    \n",
    "    ```np.std(vals)```\n",
    "    \n",
    "    * Mean first order difference of spectral centroid and spectral flatness\n",
    "     \n",
    "     ```np.mean(np.diff(vals,1))) ```\n",
    "    \n",
    "    \n",
    "* Remove the ``minmax_scaling`` from the dataset and try training the model again, are you able to get the same loss? Why do you think this might be?\n",
    "\n",
    "\n",
    "### Training \n",
    "\n",
    "* Try experimenting with the following different parameters of the network. How does it effect the final accuracy, the speed of improvement and the amount improved each time?\n",
    "\n",
    "    * learning rate\n",
    "    \n",
    "    * number of hidden neurons in the hidden layer\n",
    "    \n",
    "    * batch size \n",
    "    \n",
    "    * number of epochs\n",
    "    \n",
    "    \n",
    "* Make an additional array (``all errors``) and use it collect all the errors over time. Plot them in the graph using PyPlot and see how the loss decreases over time given different parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
