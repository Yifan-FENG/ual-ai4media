{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627d71e2",
   "metadata": {},
   "source": [
    "# Intro to Tensorflow/Keras and Tensors\n",
    "Notebook originally by Josh Murr 2022, updated by Terence Broad 2023.\n",
    "\n",
    "A few words about the course: this course assumes a certain level of programming knowledge by now, but don't worry I won't assume too much. Just that you have _some_ basic terminology understood (arrays, loops, variables etc.) and also that you have _some_ experience more specifically programming in __Python__ and that you have used packages such as __NumPy__ and __Matplotlib__ - but what is more important is that you know how to import a Python package and use it. If you are a modular student who is new to the world of Python and feel a bit lost, get in touch with Terence Broad and he can show you the ropes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30f669",
   "metadata": {},
   "source": [
    "## Installing TensorFlow / Keras\n",
    "### Option A: If you are an M1 Mac user:\n",
    "\n",
    "This is our first time using tensorflow. You will need to do this a special way.\n",
    "\n",
    "1. Open a terminal window\n",
    "2. Activate your conda environment, e.g. by typing `conda activate nlp`\n",
    "3. Install tensorflow dependencies for Mac by typing `conda install -c apple tensorflow-deps`\n",
    "4. Install the Mac OS version of tensorflow by typing `python -m pip install tensorflow-macos==2.9`\n",
    "5. Install tensorflow-metal by typing `python -m pip install tensorflow-metal==0.5.0`\n",
    "6. Close this notebook. Kill the current notebook process using Ctrl+C (or just closing all terminal windows). Then open a new terminal and launch a new notebook using the `jupyter notebook` command\n",
    "\n",
    "Then continue on where it says \"Everyone: Continue here.\" below.\n",
    "\n",
    "\n",
    "### OR, Option B. If you are NOT an M1 Mac user:\n",
    "\n",
    "Run `!pip install tensorflow` in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8494bf",
   "metadata": {},
   "source": [
    "### OR, I don't know what an M1 Mac is:\n",
    "\n",
    "If this is the case, speak to Terence and he will help you figure out what kind of computer you have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984ffca",
   "metadata": {},
   "source": [
    "## Everyone: Continue here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install keras if you haven't already\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4fbbd",
   "metadata": {},
   "source": [
    "### Lets import some libraries we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ccb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcfce5",
   "metadata": {},
   "source": [
    "## What is a Tensor?\n",
    "\n",
    "First... \n",
    "\n",
    "### 1. A Number\n",
    "\n",
    "I'm sure you know what a __number__ is... like `1`, or `10`, or `284.38`. A single lonely number out on it's own.\n",
    "\n",
    "In Python we can assign a number to a variable like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonely_number = 101 # Sorry, this is just for completeness :P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8368e1c9",
   "metadata": {},
   "source": [
    "### 2. An Array\n",
    "\n",
    "I'm also assuming you know what an __array__ is... Well, an array is just a list of __numbers__ like so: `[1, 10, 284.38]`. In mathematics an array is called a __vector__. Given that machine learning is pretty much a hybrid study of computer science and maths, there is a lot of crossover with terminology - but if you hear __vector__ think __array__.\n",
    "\n",
    "> Yes, technically, with a [loosely typed language](https://www.computerhope.com/jargon/l/looslang.htm) like Python you could have an array with anything in it, not just numbers, but lets try and be mathsy and just focus on numbers for now.\n",
    "\n",
    "In Python an array is declared with square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d833f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_or_array = [1, 10, 285.38]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2fcf79",
   "metadata": {},
   "source": [
    "### 3. A Matrix\n",
    "\n",
    "I think you also probably know what a __matrix__ is? In laymans terms, it's some kind of rectangular data structure which holds more numbers..? Like so:\n",
    "\n",
    "![A Matrix](./images/a-matrix.svg)\n",
    "\n",
    "Another way of thinking about a matrix is that it a __list of arrays__ (or vectors). So we could think of the previous matrix as a list of __3__ arrays of __length 2__:\n",
    "\n",
    "![Row Major Matrix](./images/row-major.svg)\n",
    "\n",
    "Or indeed as a list of __2__ arrays of __length 3__:\n",
    "\n",
    "![Row Major Matrix](./images/col-major.svg)\n",
    "\n",
    "Mathematically this is irrelevant, but we work with computers and so need to think about how the computer will store and work with these numbers and so becomes quite important. With Python, NumPy, Tensorflow, Keras etc. 9 times in 10 we only need to think about the __former__ which is known as __row major__ (while the latter is __column major__).\n",
    "\n",
    "So in Python notation we could declare our matrix like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = [[-1.3, 0.6],\n",
    "       [20.4, 5.5],\n",
    "       [ 9.7,-6.2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec5955",
   "metadata": {},
   "source": [
    "See how each __row__ in our matrix is just an __array__ of length 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Row 0: {mat[0]}\")\n",
    "print(f\"Row 1: {mat[1]}\")\n",
    "print(f\"Row 2: {mat[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd26bbec",
   "metadata": {},
   "source": [
    "The __length__ of the matrix is the __total number of rows__, while the length of any row is the __total number of columns__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(mat)\n",
    "num_cols = len(mat[0])\n",
    "\n",
    "print(f\"Our matrix has {num_cols} columns and {num_rows} rows!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05806271",
   "metadata": {},
   "source": [
    "It is important to feel confident in forming arrays in code and also accessing particular elements of those arrays. Honestly, it's what you end up doing _most of the time_ when peeking into datasets, creating datasets, cleaning datasets, exploring the output of a model, looking at the weights of a model.... __Everything is in arrays of arrays__. Which leads neatly onto:\n",
    "\n",
    "### 4. A Tensor\n",
    "\n",
    "Recap: \n",
    "\n",
    "- A number can be thought of as a __0-Dimensional array__ (a point, a single number): `1`\n",
    "- A series of numbers, or a vector, is a __1-Dimensional array__: `[1, 2, 3, 4, 5]`\n",
    "- A matrix is a series of arrays, which is a __2-Dimensional array__: `[[1,2],[3,4]]`\n",
    "- So... a Tensor... is a series of matrices! Or an __N-Dimensional array__. You can think of it as stacked matrices:\n",
    "\n",
    "![A Tensor](./images/tensor.svg)\n",
    "\n",
    "So if we look again at our single matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = [[-1.3, 0.6],\n",
    "       [20.4, 5.5],\n",
    "       [ 9.7,-6.2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2170ac5",
   "metadata": {},
   "source": [
    "We now want to stack three of those on top of one another.. How? Well we just need to add another dimension to our list (I will admit this is where it can get confusing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = [[[-1.3, 0.6],\n",
    "           [20.4, 5.5],\n",
    "           [ 9.7,-6.2]],\n",
    "          [[-1.3, 0.6],\n",
    "           [20.4, 5.5],\n",
    "           [ 9.7,-6.2]],\n",
    "          [[-1.3, 0.6],\n",
    "           [20.4, 5.5],\n",
    "           [ 9.7,-6.2]]]\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559b9dd",
   "metadata": {},
   "source": [
    "Now, I appreciate this is not easy to read. It is rare that you will ever need to declare a tensor like this, but it is important to understand this structure. All you really need to be able to do is to understand which __dimension__ refers to what, and how you can index each dimension to get to the data you need.\n",
    "\n",
    "I'm going to make a simpler tensor for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tensor = [[[0, 0],\n",
    "                  [0, 0],\n",
    "                  [0, 0]],\n",
    "                 [[1, 1],\n",
    "                  [1, 1],\n",
    "                  [1, 1]],\n",
    "                 [[2, 2],\n",
    "                  [2, 2],\n",
    "                  [2, 2]]]\n",
    "\n",
    "print(simple_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e91ee",
   "metadata": {},
   "source": [
    "The first dimension, in this case, refers to each particular matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simple_tensor[0])\n",
    "print(simple_tensor[1])\n",
    "print(simple_tensor[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f27849",
   "metadata": {},
   "source": [
    "The second dimension is then the __row__ is that matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1736fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simple_tensor[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8c128",
   "metadata": {},
   "source": [
    "The third dimension is then the __column__ in that row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simple_tensor[0][1][1]) # Col 1, of row 1, of matrix 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50c90c",
   "metadata": {},
   "source": [
    "Given than tensors are __N-Dimensional__, there really is no limits to the number of dimensions one could have. The best advice I can give right now is to just try and think in terms of stacked matrices, and then possibly stacked tensors.\n",
    "\n",
    "Lets look at this same example in NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our simple_tensor into a NumPy array\n",
    "np_tensor = np.array(simple_tensor)\n",
    "\n",
    "print(np_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f51c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54d5ff",
   "metadata": {},
   "source": [
    "Just to illustrate the point, here are some big __N__-Dimensional tensors (or, arrays).\n",
    "\n",
    "> NB. The first argument to `np.empty()` is the  _shape_ of the N-d array you want to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b64ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.empty((64,16,16,3))\n",
    "print(a.shape)\n",
    "\n",
    "b = np.empty((1,1,1,1,8,1))\n",
    "print(b.shape)\n",
    "\n",
    "# The following will give you a memory error! It did for me any way. \n",
    "\n",
    "# c = np.empty((8,8,8,8,8,128,256,8,8))\n",
    "\n",
    "# It is a BIG N-d array! Each entry is a 64bit Float so we are trying to allocate:\n",
    "# 8*8*8*8*8*128*256*8*8*64(bits) = 4.398*10^12 bits ~= 512 gigabytes, lol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad2f6ad",
   "metadata": {},
   "source": [
    "OK this might seem a bit boring but honestly, you're messing about with N-d arrays probably 90% of the time when working with ML frameworks or datasets.\n",
    "\n",
    "Here are a few more NumPy array methods which will come in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100414a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.empty does not initialise values, which technically can be faster,\n",
    "# but you get whatever junk is left in memory at that location.\n",
    "empty = np.empty((10,2))\n",
    "print(empty)\n",
    "\n",
    "# np.zeros/np.ones fills the values with 0's or 1's respectively.\n",
    "zeros = np.zeros((2,3))\n",
    "print(zeros)\n",
    "ones = np.ones((2,3))\n",
    "print(ones)\n",
    "\n",
    "# .fill lets you fill an array with whatever you want\n",
    "my_fill = np.empty((4,2))\n",
    "my_fill.fill(28)\n",
    "print(my_fill)\n",
    "\n",
    "# np.full combines the above 2 operations\n",
    "full = np.full((2, 4), 32)\n",
    "print(full)\n",
    "\n",
    "# You can normally add the dtype argument to any NumPy array to specify\n",
    "# the datatype you need. Things like float, uint8, float64, int... etc.\n",
    "# Can you guess why the array is full of 44's despite me specifying 300...?\n",
    "uint8_array = np.full((3, 2), 300, dtype=np.uint8)\n",
    "print(uint8_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a2aaa",
   "metadata": {},
   "source": [
    "Take a look at the [NumPy array creation routines for many more.](https://numpy.org/doc/stable/reference/routines.array-creation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791525d9",
   "metadata": {},
   "source": [
    "> ### Task!\n",
    "> Open up `./tasks/01_building_n-d_arrays.ipynb` and work through the tasks there.\n",
    ">\n",
    "> They get progressively harder, naturally, and some may seem a little off topic, but they're all good exercises to get you better at working with N-d arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf9ba7",
   "metadata": {},
   "source": [
    "## What does a machine learning library like Tensorflow do?\n",
    "\n",
    "> We're going to be using Tensorflow and Keras for much of this course. But to be honest all ML frameworks seem to be converging on a similar workflow so skills you learn in one framework are transferrable.\n",
    "\n",
    "At it's heart Tensorflow/PyTorch/Keras are just frameworks for manipulating N-d arrays, much like NumPy. NumPy is really great, so why do we just use that?\n",
    "\n",
    "Well, ML frameworks give us a few more things:\n",
    "\n",
    "- They can leverage hardware accelerators such as GPUs and TPUs.\n",
    "- They can automatically compute the gradient of arbitrary differentiable tensor expressions.\n",
    "- They allow computation to be distributed to large numbers of devices on a single machine, and large number of machines (potentially with multiple devices each).\n",
    "\n",
    "Until we start making models, working with tensors in any ML framework is a lot like working with NumPy. But the syntax can be a bit more obtuse.\n",
    "\n",
    "It is rare that you will actually need to explicitly be declaring tensors. But it is good to know how, and also how to access parts of tensors. It also highlights some fundamental details about how these frameworks actually work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077378c8",
   "metadata": {},
   "source": [
    "You can declare a tensor in Tensorflow in a number of ways, much like NumPy. Something to bare in mind is that a Tensorflow tensor holds much more information about its state than a simple N-d array which is what allows us to build complex computation graphs (models) and hand it over to Tensorflow to perform complex things like auto-differentiation and backpropagation.\n",
    "\n",
    "So with that in mind, some values with be mutable (changeable), while others should be immutable (never change), therefore we can declare a tensor as a `constant` or `variable`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8dd2e6",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_tensor = tf.constant([[5, 2], [1, 3]])\n",
    "print(constant_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use familiar methods:\n",
    "print(constant_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and access elements:\n",
    "print(constant_tensor[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa764f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that even individual elements in the tensor are also tensors!\n",
    "# To get the simple number we use .numpy() to convert to a NumPy array:\n",
    "value = constant_tensor[0][1].numpy()\n",
    "print(value)\n",
    "\n",
    "# You can also use the following syntax if you prefer:\n",
    "value = constant_tensor[0, 1].numpy()\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6dd22d",
   "metadata": {},
   "source": [
    "Much like NumPy we can also declare tensors with `.ones()` and `.zeros()`.\n",
    "\n",
    "> NB. These methods return constant tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.ones(shape=(2, 1)))\n",
    "print(tf.zeros(shape=(2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531f6f1",
   "metadata": {},
   "source": [
    "Tensorflow also has its fair share of random functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94522143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random numbers from a random normal/Gaussian distribution:\n",
    "a = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n",
    "print(a)\n",
    "\n",
    "# Random numbers from a uniform distribution of specified limits:\n",
    "b = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ad1c4",
   "metadata": {},
   "source": [
    "#### Variables\n",
    "\n",
    "Variables are special tensors used to store mutable state (such as the weights of a neural network). You create a Variable using some initial value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "my_variable = tf.Variable(initial_value)\n",
    "print(my_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a88001",
   "metadata": {},
   "source": [
    "You update the value of a Variable by using the methods .assign(value), .assign_add(increment), or .assign_sub(decrement):\n",
    "\n",
    "> Again, you probably will never do this, but Tensorflow is doing this kinda thing behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value = tf.random.normal(shape=(2, 2))\n",
    "my_variable.assign(new_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert my_variable[i, j] == new_value[i, j]\n",
    "\n",
    "added_value = tf.random.normal(shape=(2, 2))\n",
    "my_variable.assign_add(added_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert my_variable[i, j] == new_value[i, j] + added_value[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c6d84",
   "metadata": {},
   "source": [
    "You cannot do the same as above on a _constant_ tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d884e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give you an error\n",
    "new_value = tf.random.normal(shape=constant_tensor.shape)\n",
    "constant_tensor.assign(new_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cfccf",
   "metadata": {},
   "source": [
    "Maths operations are again very similar to how you would do them with NumPy. Remember these operations are being performs on N-d arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0090e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.random.normal(shape=(2, 2))\n",
    "e = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "f = d + e\n",
    "h = tf.square(f)\n",
    "i = tf.exp(h)\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c89e18",
   "metadata": {},
   "source": [
    "The element-wise nature of the operations might be more clear like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1,2],[3,4]])\n",
    "b = tf.constant([[5,6],[7,8]])\n",
    "\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5878a",
   "metadata": {},
   "source": [
    "Whereas a matrix-multiplication looks like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6149da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a @ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe7957",
   "metadata": {},
   "source": [
    "#### Gradients\n",
    "\n",
    "Here's another big difference with NumPy: you can automatically retrieve the gradient of any differentiable expression. Performing calculus on the tensors in our network is used by our gradient descent algorithms to backpropagate the error signal through the model, and therefore update the weights in training. \n",
    "\n",
    "Being able to track which tensor connects to which, and then passing the gradients backwards through the model is the key difference between variables in TensorFlow and NumPy. If this all seems a bit complicated don't worry, it can be, but don't worry TensorFlow is designed to handle this all for us, so we usually don't have to think about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cb5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(d)\n",
    "b = tf.Variable(e)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('aim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0933a3de2cf080904afa28ad0cf0619c10754b388d0b3455891742dca248a3a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
